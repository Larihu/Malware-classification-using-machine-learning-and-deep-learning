
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
import pandas as pd
import seaborn as sns
from pyDeepInsight.image_transformer import ImageTransformer
from early_stopping.pytorchtools import EarlyStopping
from sklearn.manifold import TSNE
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
import torch
import torchvision.transforms as transforms
import torchvision.models as models
from torch.utils.data import TensorDataset, DataLoader, random_split
import torch.nn as nn
import torch.optim as optim
import time

import warnings
warnings.filterwarnings("ignore")


"""Read dataset"""
df=pd.read_csv('../datasets/drebin-215-dataset-5560malware-9476-benign.csv')
print(df.columns)

"""Clean dataset"""
df = df.replace('?', np.nan)
df.isnull().values.any()
df = df.dropna()

"""Splitting dataset feature"""
# Splitting dataset features into X and y
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

# Splitting the dataset into Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42, stratify=y)

"""Tranformation with DeepInsight"""

transformer = FunctionTransformer(np.log1p, validate=True)
X_train_norm = transformer.transform(X_train)
X_test_norm = transformer.transform(X_test)

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc = le.transform(y_test)
num_classes = np.unique(y_train_enc).size

"""Create t-SNE object"""
distance_metric = 'cosine'
reducer = TSNE(
    n_components=2,
    metric=distance_metric,
    square_distances=True,
    n_jobs=-1
)

""" We will train the image transformer on the training data and then transform training and testing sets. Values should be between 0 and 1. """

pixel_size = (224,224)
it = ImageTransformer(
    feature_extractor=reducer, 
    pixels=pixel_size)

it.fit(X_train, y=y_train, plot=False)
X_train_img = it.transform(X_train_norm)
X_test_img = it.transform(X_test_norm)

print("[INFO] Image trnasformer completed")

fdm = it.feature_density_matrix()
fdm[fdm == 0] = np.nan

print("[INFO] Feature matrix created")

plt.figure(figsize=(10, 7.5))
plt.savefig('../Images/plot1.png')

ax = sns.heatmap(fdm, cmap="viridis", linewidths=0., 
                 linecolor="lightgrey", square=True)
ax.xaxis.set_major_locator(ticker.MultipleLocator(5))
ax.yaxis.set_major_locator(ticker.MultipleLocator(5))
for _, spine in ax.spines.items():
    spine.set_visible(True)
_ = plt.title("Genes per pixel")

fig, ax = plt.subplots(1, 3, figsize=(15, 5))
for i in range(0,3):
    ax[i].imshow(X_train_img[i])
    ax[i].title.set_text(f"Train[{i}] - class '{y_train[i]}'")
plt.tight_layout()
plt.savefig('../Images/plot2.png')

X_test_img = it.transform(X_test_norm)

fig, ax = plt.subplots(1, 3, figsize=(15, 5))
for i in range(0,3):
    ax[i].imshow(X_test_img[i])
    ax[i].title.set_text(f"Test[{i}] - class '{y_test[i]}'")
plt.tight_layout()
plt.savefig('../Images/plot3.png')


"""Create CNN"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

"""Transform numpy image format to PyTorch tensor."""
preprocess = transforms.Compose([
    transforms.ToTensor()
])

X_train_tensor = torch.stack([preprocess(img) for img in X_train_img]).float().to(device)
y_train_tensor = torch.from_numpy(le.fit_transform(y_train)).to(device)

X_test_tensor = torch.stack([preprocess(img) for img in X_test_img]).float().to(device)
y_test_tensor = torch.from_numpy(le.transform(y_test)).to(device)


"""Build model"""
model = models.resnet18(pretrained=True)

# Specify own classifier.

model.classifier = nn.Sequential(
            torch.nn.Conv2d(32, 32, kernel_size=3),
            torch.nn.ReLU(),
            torch.nn.BatchNorm2d(32),
            torch.nn.Conv2d(32, 32, kernel_size=3),
            torch.nn.ReLU(),
            torch.nn.BatchNorm2d(32),
        )

# Specify the loss function.
criterion = nn.CrossEntropyLoss()

# Specify the optimizer to optimize the weights and biases.
optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)

model.to(device)

print("[INFO] Model built")


"""Generate datasets and dataloaders for training and testing sets."""

batch_size = 64

trainset = TensorDataset(X_train_tensor, y_train_tensor)
testset = TensorDataset(X_test_tensor, y_test_tensor)

# calculate the train/validation split
print("[INFO] generating the train/validation split...")
numTrainSamples = int((len(trainset) * 0.75)+1)
numValSamples = int(len(trainset) * 0.25)

(trainset, validset) = random_split(trainset,
	[numTrainSamples, numValSamples],
	generator=torch.Generator().manual_seed(42))

trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)
validloader = DataLoader(validset, batch_size=batch_size, shuffle=False)


# calculate steps per epoch for training and validation set
trainSteps = len(trainloader.dataset) // batch_size
valSteps = len(validloader.dataset) // batch_size
testSteps = len(testloader.dataset) // batch_size

print("[INFO] Datasets and dataloaders for training and testing sets generated")

"""Train model"""

epochs = 10
patience = 5

# initialize a dictionary to store training history
H = {
	"train_loss": [],
	"train_acc": [],
	"val_loss": [],
	"val_acc": []
}

# initialize the early_stopping object
early_stopping = EarlyStopping(patience=patience, verbose=True)

# measure how long training is going to take
print("[INFO] training the network...")
startTime = time.time()

 # for loop to train model.
for e in range(0, epochs):

    # Set model in training mode.
    model.train() 

    # initialize the total training and validation loss
    totalTrainLoss = 0
    totalValLoss = 0

	# initialize the number of correct predictions in the training
	# and validation step
    trainCorrect = 0
    valCorrect = 0

    # for loop to iterate through training dataloader.
    for inputs, labels in trainloader:
        
        # Move input and label tensors to the default device.
        inputs, labels = inputs.to(device), labels.to(device)
        # Forward pass, back propagation, gradient descent and updating weights and bias.
        # Forward pass through model to get log of probabilities.
        pred = model(inputs)
        # Calculate loss of model output based on model prediction and labels.
        loss = criterion(pred, labels)
        # Set gradients to 0 to avoid accumulation
        optimizer.zero_grad()
        # Back propagation of loss through model / gradient descent.
        loss.backward()
        # Update weights / gradient descent.
        optimizer.step()
        
        # add the loss to the total training loss so far and
		# calculate the number of correct predictions
        totalTrainLoss += loss
        trainCorrect += (pred.argmax(1) == labels).type(
			torch.float).sum().item()

    # Turn off gradients for validation, saves memory and computations.
    with torch.no_grad():
            # Set model to evaluation mode to turn off dropout so all images in the validation & test set are passed through the model.
        model.eval()
        # for loop to evaluate loss of validation image set and its accuracy.
        for valid_inputs, valid_labels in validloader:
            # Move input and label tensors to the default device.
            valid_inputs, valid_labels = valid_inputs.to(device), valid_labels.to(device)
            
            # Run validation image set through model.
            valid_pred = model(valid_inputs)
            
            # Calculate loss and accumulate it for validation image set.
            totalValLoss += criterion(valid_pred, valid_labels)
            
            # calculate the number of correct predictions
            valCorrect += (valid_pred.argmax(1) == valid_labels).type(
                torch.float).sum().item()

    # calculate the average training and validation loss
    avgTrainLoss = totalTrainLoss / trainSteps
    avgValLoss = totalValLoss / valSteps
    # calculate the training and validation accuracy
    trainCorrect = trainCorrect / len(trainloader.dataset)
    valCorrect = valCorrect / len(validloader.dataset)
    # update our training history
    H["train_loss"].append(avgTrainLoss.cpu().detach().numpy())
    H["train_acc"].append(trainCorrect)
    H["val_loss"].append(avgValLoss.cpu().detach().numpy())
    H["val_acc"].append(valCorrect)
    # print the model training and validation information
    print("[INFO] EPOCH: {}/{}".format(e + 1, epochs))
    print("Train loss: {:.6f}, Train accuracy: {:.4f}".format(
        avgTrainLoss, trainCorrect))
    print("Val loss: {:.6f}, Val accuracy: {:.4f}\n".format(
        avgValLoss, valCorrect))

    early_stopping(totalValLoss, model)
    if early_stopping.early_stop:
        print("Early stopping")
        break

# finish measuring how long training took
endTime = time.time()
print("[INFO] total time taken to train the model: {:.2f}s".format(
	endTime - startTime))

# we can now evaluate the network on the test set
print("[INFO] evaluating network...")

totalTestLoss = 0
testCorrect = 0

# Turn off gradients for testing, saves memory and computations.
with torch.no_grad():
    # Set model to evaluation mode to turn off dropout so all images in the test set are passed through the model.
    model.eval()
    # for loop to evaluate loss of test image set and its accuracy.
    for test_inputs, test_labels in testloader:
        # Move input and label tensors to the default device.
        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)
        
        # Run test image set through model.
        test_pred = model(test_inputs)
        
        # Calculate loss and accumulate it for test image set.
        totalTestLoss += criterion(test_pred, test_labels)
        
        # calculate the number of correct predictions
        testCorrect += (test_pred.argmax(1) == test_labels).type(
            torch.float).sum().item()

# calculate the average training and validation loss
avgTestLoss = totalTestLoss / testSteps
# calculate the training and validation accuracy
testCorrect = testCorrect / len(testloader.dataset)

print("Test loss: {:.6f}, Test accuracy: {:.4f}\n".format(
        avgTestLoss, testCorrect))












"""

Generate pyTorch datasets and dataloaders for training and testing sets.

batch_size = 256

trainset = TensorDataset(X_train_tensor, y_train_tensor)
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)

validset = TensorDataset(X_test_tensor, y_test_tensor) 
validloader = DataLoader(validset, batch_size=batch_size)

testset = TensorDataset(X_test_tensor, y_test_tensor)
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)

print("Done -> Generate pyTorch datasets and dataloaders for training and testing sets.")

Train model

epochs = 100
running_loss = 0
list_running_loss, list_valid_loss, list_valid_accuracy = [], [], []

# initialize the early_stopping object
early_stopping = EarlyStopping(patience=20, verbose=True)

 # for loop to train model.
for epoch in range(epochs):
    # for loop to iterate through training dataloader.
    for inputs, labels in trainloader:
        
        # Move input and label tensors to the default device.
        inputs, labels = inputs.to(device), labels.to(device)
        
        # Set gradients to 0 to avoid accumulation
        optimizer.zero_grad()
        
        # Forward pass, back propagation, gradient descent and updating weights and bias.
        # Forward pass through model to get log of probabilities.
        log_ps = model.forward(inputs)
        # Calculate loss of model output based on model prediction and labels.
        loss = criterion(log_ps, labels)
        # Back propagation of loss through model / gradient descent.
        loss.backward()
        # Update weights / gradient descent.
        optimizer.step()
        
        # Accumulate loss for training image set for print out in terminal
        running_loss += loss.item()

        # Calculate loss for verification image set and accuracy for print out in terminal.
        # Validation pass and print out the validation accuracy.
        # Set loss of validation set and accuracy to 0.
        valid_loss = 0
        valid_accuracy = 0
        
        # Set model to evaluation mode to turn off dropout so all images in the validation & test set are passed through the model.
        model.eval()
        
        # Turn off gradients for validation, saves memory and computations.
        with torch.no_grad():
            # for loop to evaluate loss of validation image set and its accuracy.
            for valid_inputs, valid_labels in validloader:
                # Move input and label tensors to the default device.
                valid_inputs, valid_labels = valid_inputs.to(device), valid_labels.to(device)
                
                # Run validation image set through model.
                valid_log_ps = model.forward(valid_inputs)
                
                # Calculate loss for validation image set.
                valid_batch_loss = criterion(valid_log_ps, valid_labels)
                
                # Accumulate loss for validation image set.
                valid_loss += valid_batch_loss.item()
                
                # Calculate probabilities
                valid_ps = torch.exp(valid_log_ps)
                
                # Get the most likely class using the ps.topk method.
                valid_top_k, valid_top_class = valid_ps.topk(1, dim=1)
                
                # Check if the predicted classes match the labels.
                valid_equals = valid_top_class == valid_labels.view(*valid_top_class.shape)
                
                # Calculate the percentage of correct predictions.
                valid_accuracy += torch.mean(valid_equals.type(torch.FloatTensor)).item()
        
        # Print out losses and accuracies
        # Create string for running_loss.
        str1 = ["Train loss: {:.3f} ".format(running_loss)]
        str1 = "".join(str1)
        # Create string for valid_loss.
        str2 = ["Valid loss: {:.3f} ".format(valid_loss/len(validloader))]
        str2 = "".join(str2)
        # Create string for valid_accuracy.
        str3 = ["Valid accuracy: {:.3f} ".format(valid_accuracy/len(validloader))]
        str3 = "".join(str3)
        # Print strings
        print(f"{epoch+1}/{epochs} " + str1 + str2 + str3)
        
        # Append current losses and accuracy to lists to print losses and accuracies.
        list_running_loss.append(running_loss)
        list_valid_loss.append(valid_loss/len(validloader))
        list_valid_accuracy.append(valid_accuracy/len(validloader))
        
        # Set running_loss to 0.
        running_loss = 0
        
        # Set model back to train mode.
        model.train() 

        
        early_stopping(valid_loss, model)
        if early_stopping.early_stop:
            print("Early stopping")
            break

"""